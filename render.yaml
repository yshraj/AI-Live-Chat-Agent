# Render.com deployment configuration
# This file makes it easier to deploy to Render.com
# See: https://render.com/docs/yaml-spec

services:
  - type: web
    name: spur-chat-backend
    env: python
    region: oregon  # Change to your preferred region
    plan: free
    buildCommand: cd backend && pip install --upgrade pip setuptools wheel && pip install torch --index-url https://download.pytorch.org/whl/cpu && pip install -r requirements.txt && pip install torch --index-url https://download.pytorch.org/whl/cpu --upgrade --force-reinstall && python -c "import torch; print(f'PyTorch version: {torch.__version__}, CUDA available: {torch.cuda.is_available()}')"
    startCommand: cd backend && uvicorn app.main:app --host 0.0.0.0 --port $PORT --workers 1 --limit-concurrency 10
    envVars:
      - key: PYTHON_VERSION
        value: 3.11.9  # Required for numpy==1.24.3 compatibility
      - key: GOOGLE_API_KEY
        sync: false  # Set this in Render dashboard
      - key: GOOGLE_MODEL
        value: gemini-2.5-flash
      - key: MONGODB_URL
        sync: false  # Set this in Render dashboard
      - key: MONGODB_DB_NAME
        value: spur_chat
      - key: UPSTASH_REDIS_REST_URL
        sync: false  # Set this in Render dashboard
      - key: UPSTASH_REDIS_REST_TOKEN
        sync: false  # Set this in Render dashboard
      - key: CORS_ORIGINS
        sync: false  # REQUIRED: Set this to your Vercel frontend URL (e.g., https://your-app.vercel.app)
        # Multiple origins: comma-separated (e.g., https://app1.vercel.app,https://app2.vercel.app)
      - key: TOKENIZERS_PARALLELISM
        value: "false"  # Disable tokenizer parallelism to reduce memory usage
      - key: OMP_NUM_THREADS
        value: "1"  # Limit OpenMP threads to reduce memory footprint
      - key: MKL_NUM_THREADS
        value: "1"  # Limit MKL threads for numpy operations

